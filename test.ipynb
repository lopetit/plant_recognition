{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanisf/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'thirdparty/pybioclip/src/'))\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "from bioclip.predict import CustomLabelsClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classifier = CustomLabelsClassifier(device=device)\n",
    "\n",
    "classes = [\n",
    "    \"Plantae Angiosperms Eudicots Asterales Asteraceae Arnica griscomii (Snow Arnica)\",\n",
    "    \"Plantae Angiosperms Eudicots Brassicales Brassicaceae Boechera quebecensis (Rockcress)\",\n",
    "    \"Plantae Angiosperms Eudicots Brassicales Brassicaceae Draba pycnosperma (Whitlow-grass)\",\n",
    "    \"Plantae Angiosperms Eudicots Asterales Asteraceae Erigeron compositus (Fleabane)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plantae Angiosperms Eudicots Asterales Asteraceae Arnica griscomii (Snow Arnica) 1.0\n",
      "Plantae Angiosperms Eudicots Brassicales Brassicaceae Boechera quebecensis (Rockcress) 2.177514613066478e-09\n",
      "Plantae Angiosperms Eudicots Brassicales Brassicaceae Draba pycnosperma (Whitlow-grass) 5.394313329354361e-10\n",
      "Plantae Angiosperms Eudicots Asterales Asteraceae Erigeron compositus (Fleabane) 3.4106719226656423e-08\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(\"./test_images/Arnica_0.png\", classes)\n",
    "for prediction in predictions:\n",
    "   print(prediction[\"classification\"], prediction[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/PIL/Image.py:3281\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3281\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m         tile \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcrop((left, upper, right, lower))\n\u001b[1;32m     31\u001b[0m         tiles\u001b[38;5;241m.\u001b[39mappend(tile)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mpredict_classifications_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding/projects/plant_project/plant_recognition/thirdparty/pybioclip/src/bioclip/predict.py:211\u001b[0m, in \u001b[0;36mpredict_classifications_from_list\u001b[0;34m(img, cls_ary, device)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_classifications_from_list\u001b[39m(img: Union[PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mstr\u001b[39m], cls_ary: List[\u001b[38;5;28mstr\u001b[39m], device: Union[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    210\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m CustomLabelsClassifier(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_ary\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding/projects/plant_project/plant_recognition/thirdparty/pybioclip/src/bioclip/predict.py:189\u001b[0m, in \u001b[0;36mCustomLabelsClassifier.predict\u001b[0;34m(self, image_path, cls_ary)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_path: \u001b[38;5;28mstr\u001b[39m, cls_ary: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m--> 189\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mopen_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     classes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cls_ary]\n\u001b[1;32m    191\u001b[0m     txt_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_txt_features(classes)\n",
      "File \u001b[0;32m~/coding/projects/plant_project/plant_recognition/thirdparty/pybioclip/src/bioclip/predict.py:123\u001b[0m, in \u001b[0;36mopen_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_image\u001b[39m(image_path):\n\u001b[0;32m--> 123\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/PIL/Image.py:3283\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3281\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n\u001b[0;32m-> 3283\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m())\n\u001b[1;32m   3284\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3286\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the image\n",
    "image_path = '../plant_dataset_Forillon/data_2023/Arnica griscomii/DJI_20220704062634_0012_SUPR_ARNICA_mgb.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define the dimensions of each tile\n",
    "tile_width = 288\n",
    "tile_height = 324\n",
    "\n",
    "# Get the dimensions of the image\n",
    "width, height = image.size\n",
    "\n",
    "# Calculate the number of tiles in each dimension\n",
    "num_tiles_x = width // tile_width\n",
    "num_tiles_y = height // tile_height\n",
    "\n",
    "# List to hold the tiles\n",
    "tiles = []\n",
    "\n",
    "# Loop through the image and create the tiles\n",
    "for i in range(num_tiles_y):\n",
    "    for j in range(num_tiles_x):\n",
    "        left = j * tile_width\n",
    "        upper = i * tile_height\n",
    "        right = left + tile_width\n",
    "        lower = upper + tile_height\n",
    "        \n",
    "        # Crop the tile and add it to the list\n",
    "        tile = image.crop((left, upper, right, lower))\n",
    "        tiles.append(tile)\n",
    "\n",
    "predict_classifications_from_list(tiles, classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-3.9.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
